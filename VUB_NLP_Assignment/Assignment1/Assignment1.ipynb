{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "- Data Set Choose: Reviews of Musical Instrument\n",
    "- The following data Musical Instrument is from http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "punctuations = string.punctuation\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Musical_Instruments_5.json.gz')\n",
    "# print(len(df.overall))\n",
    "# print(df.reviewText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Method 1: \n",
    "- Like binary classification, the approach can be extended to multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# part of the code inspired by : https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Text%20Classification%20With%20Machine%20Learning,SpaCy,Sklearn(Sentiment%20Analysis)/Text%20Classification%20&%20Sentiment%20Analysis%20with%20SpaCy,Sklearn.ipynb\n",
    "\n",
    "def scorePreprocessor(score):\n",
    "    # 1  -> positive \n",
    "    # -1 -> negative\n",
    "    # 0  -> neutral \n",
    "    res = []\n",
    "    for i in range(len(score)):\n",
    "        if score[i] > 3.0:\n",
    "            res.append(1)\n",
    "        elif score[i] < 3.0:\n",
    "            res.append(-1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return res\n",
    "    \n",
    "def handleNegation(review):\n",
    "    length = len(review)\n",
    "    i = 0\n",
    "    negation_String = 'NOT_'\n",
    "    for i in range(len(review)):\n",
    "        if review[i] in [\"not\", \"n't\", \"not\"] and i < len(review) -1 :\n",
    "            # adding negation String\n",
    "            i = i + 1\n",
    "            while review[i] not in punctuations and i < len(review) -1:\n",
    "                review[i] = negation_String + review[i]\n",
    "                i = i + 1\n",
    "    return review\n",
    "            \n",
    "\n",
    "def textNormalization(review):\n",
    "    # tokenization and lemmatizing\n",
    "    stop_words = list(STOP_WORDS)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(review)):\n",
    "        mytokens = nlp(review[i])\n",
    "        mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "        mytokens = handleNegation(mytokens) \n",
    "        # Filtering out punctuations\n",
    "        mytokens = [ word for word in mytokens if word not in punctuations ]  \n",
    "        result.append(mytokens)\n",
    "    \n",
    "    return result\n",
    "    #print(np.shape(result))\n",
    "    #print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traning the sentiment analyser\n",
    "- First Method, SVM and Naive Bayes Classifier\n",
    "- Remember : Use Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# def N_gram(result):\n",
    "#     # Use the zip function to help us generate n-grams\n",
    "#     # Concatentate the tokens into ngrams and return\n",
    "#     ngrams = zip(*[result[i:] for i in range(n)])\n",
    "#     return [\" \".join(ngram) for ngram in ngrams]\n",
    "    \n",
    "\n",
    "def stupidBackTosetence(result):\n",
    "    new_result = []\n",
    "    for i in range(len(result)):\n",
    "        sentence = result[i][0]\n",
    "        for j in range(1, len(result[i])):\n",
    "            sentence = sentence + \" \" + result[i][j]\n",
    "        new_result.append(sentence)\n",
    "    return new_result\n",
    "\n",
    "\n",
    "def vectorizeAndNgram(result):\n",
    "    # verctorize and get feature using bi-gram\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                analyzer='word',\n",
    "                ngram_range=(2, 2),\n",
    "                max_features=30000)\n",
    "\n",
    "    # fit_transform on to get the features\n",
    "    feature = vectorizer.fit_transform(result)\n",
    "    return feature\n",
    "\n",
    "\n",
    "# splitting data to 80% training_set 10% test_set and 10 %dev_set\n",
    "def dataSplit(X_all, y_all):\n",
    "    training_size = np.int(X_all.shape[0] * 0.8)\n",
    "    print(training_size)\n",
    "    test_size = np.int(X_all.shape[0] * 0.1)\n",
    "    X_train = X_all[0:training_size]\n",
    "    X_test = X_all[training_size :training_size + test_size]\n",
    "    X_dev = X_all[training_size + test_size:]\n",
    "    \n",
    "    y_train = y_all[0:training_size]\n",
    "    y_test = y_all[training_size :training_size + test_size]\n",
    "    y_dev = y_all[training_size + test_size:]\n",
    "                                                   \n",
    "    #print(np.shape(y_dev))                                            \n",
    "    return X_train, y_train, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "scoreTarget = scorePreprocessor(df.overall[0:100])\n",
    "review_norm = textNormalization(df.reviewText[0:100])\n",
    "review = stupidBackTosetence(review_norm)\n",
    "review_processed = vectorizeAndNgram(review)\n",
    "X_train, y_train, X_test, y_test, X_dev, y_dev = dataSplit(review_processed, scoreTarget)\n",
    "\n",
    "\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(X_dev))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
